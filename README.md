# ğŸ”¥ Sparkâ€“Kafkaâ€“MySQL Observability Stack (E-PAY 2.0)
# ğŸš€ Spark-Kafka-MySQL ETL Pipeline with Airflow & Loki/Grafana (E-PAY 2.0 POC)

This repository contains a **complete end-to-end data engineering stack** built using Docker Compose â€” integrating **Apache Spark**, **Kafka (KRaft mode)**, **MySQL**, **Apache Airflow**, and **Grafana Loki/Promtail** for centralized logging and monitoring.

Designed and implemented as part of the **E-PAY 2.0 project** â€” for SBI in collaboration with Red Hat â€” this POC demonstrates real-time data ingestion, ETL processing, job orchestration, and log visualization in a containerized environment.

---

## ğŸ—ï¸ **Architecture Overview**
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚         Apache Airflow         â”‚
           â”‚   (Job Orchestrator & DAG UI)  â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚ triggers
                          â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                 Apache Spark                 â”‚
    â”‚  spark-master + spark-worker + history-serverâ”‚
    â”‚  runs ETL jobs via JDBC over MySQL           â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚    MySQL     â”‚
             â”‚ Source + Sinkâ”‚
             â”‚ (table1, table2, result) â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚   Apache Kafka     â”‚
           â”‚ (KRaft mode broker)â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚ Loki + Promtail + Grafana â”‚
           â”‚   Centralized Log View UI â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜





---

## ğŸ§° **Tech Stack**

| Component | Description |
|------------|-------------|
| **Apache Spark 4.0.0** | ETL engine for comparing and transforming large datasets |
| **Apache Kafka 3.7.0 (KRaft)** | Message broker (no Zookeeper) |
| **MySQL 8.0.43** | Source and sink database for ETL |
| **Apache Airflow 2.10.2** | Workflow orchestration for Spark jobs |
| **Grafana + Loki + Promtail** | Monitoring and log aggregation |
| **Docker Compose** | Container orchestration for all services |

---

## âš™ï¸ **1. Prerequisites**

- Docker â‰¥ 24.x  
- Docker Compose â‰¥ 2.x  
- Minimum 8 GB RAM (recommended 12 GB for full stack)
- Linux/WSL2 environment

---

## ğŸ§© **2. Repository Structure**
spark-kafka-docker/
â”œâ”€â”€ airflow/
â”‚ â”œâ”€â”€ dags/
â”‚ â”‚ â”œâ”€â”€ compare_tables_dag.py # Airflow DAG to run Spark ETL
â”‚ â”‚ â””â”€â”€ spark_kafka_mysql_dag.py # Kafka â†’ Spark â†’ MySQL DAG
â”‚ â”œâ”€â”€ logs/
â”‚ â””â”€â”€ plugins/
â”‚
â”œâ”€â”€ dags/ # Optional DAG mounts
â”œâ”€â”€ loki/config.yaml # Loki configuration
â”œâ”€â”€ promtail/config.yml # Promtail log collection config
â”‚
â”œâ”€â”€ table_compare_etl.py # PySpark ETL script (table1 vs table2)
â”‚
â”œâ”€â”€ docker-compose.yml # Spark + Kafka + MySQL stack
â”œâ”€â”€ docker-compose.airflow.yml # Airflow orchestration layer
â”œâ”€â”€ docker-compose.loki.yml # Loki, Promtail & Grafana monitoring
â”‚
â”œâ”€â”€ jars/ # MySQL JDBC connector (if needed)
â”œâ”€â”€ logs/ # Local log mounts
â””â”€â”€ README.md



---

## ğŸ³ **3. Bring up the environment**

### Step 1 â€“ Start core stack (Spark + Kafka + MySQL)
```bash
docker-compose up -d


Step 2 â€“ Start Airflow stack

docker-compose -f docker-compose.airflow.yml up -d


Step 3 â€“ Start Loki + Promtail + Grafana
Step 3 â€“ Start Loki + Promtail + Grafana
docker-compose -f docker-compose.loki.yml up -d

check containers
docker ps --format "table {{.Names}}\t{{.Status}}\t{{.Ports}}"


4. MySQL Database Setup

Connect to MySQL:

docker exec -it mysql mysql -usparkuser -psparkpass testdb

Create tables for ETL:
Create tables for ETL:
CREATE TABLE table1 (id INT, name VARCHAR(50), amount INT);
CREATE TABLE table2 (id INT, name VARCHAR(50), amount INT);
CREATE TABLE table_compare_result AS SELECT * FROM table1 WHERE 1=0;

INSERT INTO table1 VALUES (1,'Alice',100), (2,'Bob',200), (3,'Charlie',300);
INSERT INTO table2 VALUES (1,'Alice',100), (2,'Bob',250), (3,'Charlie',300);


âš¡ 5. ETL Job: Compare Two Tables

table_compare_etl.py uses Spark to:

Compare two large tables (table1, table2)

Identify mismatched or missing rows

Store results into table_compare_result

docker exec -it spark-master \
  spark-submit \
  --master spark://spark-master:7077 \
  --jars /opt/spark/jars/mysql-connector-java-8.0.33.jar \
  /opt/spark/work-dir/table_compare_etl.py


ğŸª¶ 6. Orchestrate via Airflow

Access Airflow Web UI:

http://localhost:8080

Username: admin
Password: admin

Enable and trigger DAG:

compare_tables_dag

check via cli
docker exec airflow-webserver airflow dags list
docker exec airflow-scheduler airflow dags trigger compare_tables_dag
docker exec airflow-scheduler airflow dags list-runs -d compare_tables_dag



ğŸ“Š 7. Logs & Monitoring

Access Grafana:

http://localhost:3000

Username: admin
Password: admin

âœ… Loki endpoint automatically configured at http://loki:3100
âœ… Promtail collects container logs from /var/lib/docker/containers

You can filter logs by container:


{container_name="spark-master"}
{container_name="airflow-webserver"}


ğŸ§  8. Validation

Check ETL output in MySQL:

docker exec -it mysql mysql -usparkuser -psparkpass testdb
SELECT * FROM table_compare_result;


Expected result:

id_a	name_a	amount_a	id_b	name_b	amount_b
2	Bob	200	2	Bob	250
ğŸ›  9. Common Commands
Action	Command
Rebuild all containers	docker-compose down -v && docker-compose up -d
View logs for a service	docker logs spark-master -f
Restart Airflow only	docker-compose -f docker-compose.airflow.yml restart
Stop everything	docker-compose down && docker-compose -f docker-compose.airflow.yml down && docker-compose -f docker-compose.loki.yml down
ğŸ“¦ 10. Future Enhancements

Add Kafka â†’ Spark streaming DAGs (CDC / real-time data diff)

Push ETL metadata to DynamoDB or S3

Enable Spark History Server S3 integration

Add Grafana dashboard JSON templates

Integrate Red Hat Service Mesh for distributed tracing

ğŸ§‘â€ğŸ’» Author



Saadi Muzzammil
DevOps / Data Engineering â€“ E-PAY 2.0 Project (SBI x Red Hat)
ğŸ“§ github.com/saadi2047

ğŸ License

MIT License Â© 2025 Saadi Muzzammil































Complete end-to-end environment for streaming ETL + monitoring:
- **Apache Spark** (master + worker + history)
- **Kafka 3.7.0 (KRaft mode)**
- **MySQL 8.0**
- **Airflow** for orchestration
- **Grafana + Loki + Promtail** for centralized logs

---

## ğŸ§© Quick Start

```bash
# Core stack
docker-compose up -d
# Airflow
docker-compose -f docker-compose.airflow.yml up -d
# Observability
docker-compose -f docker-compose.loki.yml up -d


ğŸŒ Access URLs
Service	URL	Notes
Spark Master	http://localhost:8081
	Spark UI
Spark Worker	http://localhost:8082
	Worker UI
History Server	http://localhost:18080
	Job logs
Airflow UI	http://localhost:8080
	Scheduler/DAGs
Grafana	http://localhost:3000
	admin/admin
Loki	http://localhost:3100
	log API



ğŸ§  Future Enhancements

Add Prometheus + Node Exporter for metrics

Deploy to OpenShift or EKS for DR testing

Integrate CI/CD via GitHub Actions



Save it, commit, and push:
```bash
git add README.md
git commit -m "ğŸ“ Updated README with observability and access info"
git push







# spark-kafka-docker
Complete Spark 4 + Kafka 3.7 + MySQL 8 + History Server setup



spark-kafka-docker/
â”œâ”€â”€ docker-compose.yml                # Spark + Kafka + MySQL core
â”œâ”€â”€ docker-compose.airflow.yml        # Airflow orchestration
â”œâ”€â”€ docker-compose.loki.yml           # Grafana + Loki + Promtail stack
â”œâ”€â”€ loki/
â”‚   â””â”€â”€ config.yaml                   # Loki v3.0.0 config (fixed)
â”œâ”€â”€ promtail/
â”‚   â””â”€â”€ config.yml                    # Promtail log collector config
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ logs/
â”‚   â””â”€â”€ plugins/
â”œâ”€â”€ dags/                             # optional DAGs dir (duplicate safe)
â”œâ”€â”€ jars/                             # Spark JARs if needed
â”œâ”€â”€ logs/                             # local container logs
â”œâ”€â”€ streaming-etl.py                  # your ETL script (Kafkaâ†’Sparkâ†’MySQL)
â”œâ”€â”€ streaming-test.py                 # your test consumer script
â”œâ”€â”€ restart-recovery.sh               # helper script for restart
â”œâ”€â”€ .gitignore                        # will add below
â””â”€â”€ README.md                         # repo documentation
